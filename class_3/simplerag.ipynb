{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader=WebBaseLoader(\n",
    "    web_paths=(\"https://www.barcelonasc.com.ec/2025/07/01/comunicado-oficial-jean-carlos-montano/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"elementor-section elementor-inner-section elementor-element elementor-element-649b8c9e elementor-section-boxed elementor-section-height-default elementor-section-height-default\")\n",
    ")))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0171d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.barcelonasc.com.ec/2025/07/01/comunicado-oficial-jean-carlos-montano/\n",
      "Barcelona Sporting Club informa que ha llegado a un acuerdo con el volante Jean Carlos Montaño, quien se incorpora a nuestra institución como agente libre.\n",
      "La operación contempla un contrato por el plazo de un año, con inicio en el mes de julio de 2025.\n",
      "El jugador, de 26 años de edad, ha superado satisfactoriamente todas las pruebas médicas y ya se encuentra a disposición del primer equipo, bajo las órdenes del director técnico Ismael Rescalvo.\n",
      "Le damos la bienvenida a Jean Carlos y le deseamos muchos éxitos defendiendo nuestros colores.\n",
      "Barcelona Sporting Club\n"
     ]
    }
   ],
   "source": [
    "print(text_documents[0].metadata['source'])\n",
    "print(text_documents[0].page_content.strip())  # Print first 1000 characters of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd8e32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('CV_ES.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dc85ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor del documento: iesus davila\n",
      "Total de páginas: 2\n",
      "==========\n",
      "Página 0\n",
      "Iesus Dávila Ingeniero Mecatrónico\n",
      "Teléfono: +593 96 314 5136\n",
      "Email: iesusdavila@gmail.com\n",
      "Dirección\n",
      "==========\n",
      "Página 1\n",
      "INVESTIGACIÓN Software Architecture and Simulation Interface for Autonomous Underwater Vehicles\n",
      "Pape\n"
     ]
    }
   ],
   "source": [
    "print(f\"Autor del documento: {docs[0].metadata['author']}\")\n",
    "print(f\"Total de páginas: {docs[0].metadata['total_pages']}\")\n",
    "\n",
    "for page in docs:\n",
    "    print(\"=\"*10)\n",
    "    print(f\"Página {page.metadata['page']}\")\n",
    "    print(f\"{page.page_content[:100]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7b7b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4f7b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor del documento: iesus davila\n",
      "Total de páginas: 2\n",
      "==========\n",
      "Página 0\n",
      "Iesus Dávila Ingeniero Mecatrónico\n",
      "Teléfono: +593 96 314 5136\n",
      "Email: iesusdavila@gmail.com\n",
      "Dirección\n",
      "==========\n",
      "Página 0\n",
      "Unilever\n",
      "Ingeniero en Robótica e IA | Febrero 2025 - Presente\n",
      "Digital Factory Intern | Agosto 2024 -\n",
      "==========\n",
      "Página 1\n",
      "INVESTIGACIÓN Software Architecture and Simulation Interface for Autonomous Underwater Vehicles\n",
      "Pape\n"
     ]
    }
   ],
   "source": [
    "print(f\"Autor del documento: {documents[0].metadata['author']}\")\n",
    "print(f\"Total de páginas: {documents[0].metadata['total_pages']}\")\n",
    "\n",
    "for page in documents:\n",
    "    print(\"=\"*10)\n",
    "    print(f\"Página {page.metadata['page']}\")\n",
    "    print(f\"{page.page_content[:100]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63b6e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3050 Laptop GPU) - 2839 MiB free\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/iesus_robot/projects/langchain-lab/models/models--mixedbread-ai--mxbai-embed-large-v1/snapshots/db9d1fe0f31addb4978201b2bf3e577f3f8900d2/gguf/mxbai-embed-large-v1-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1\n",
      "llama_model_loader: - kv   2:                           bert.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                        bert.context_length u32              = 512\n",
      "llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096\n",
      "llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv   9:                      bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\n",
      "llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\n",
      "llama_model_loader: - type  f32:  243 tensors\n",
      "llama_model_loader: - type  f16:  146 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 637.85 MiB (16.02 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 3\n",
      "load: control token:    101 '[CLS]' is not marked as EOG\n",
      "load: control token:    103 '[MASK]' is not marked as EOG\n",
      "load: control token:      0 '[PAD]' is not marked as EOG\n",
      "load: control token:    100 '[UNK]' is not marked as EOG\n",
      "load: control token:    102 '[SEP]' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 5\n",
      "load: token to piece cache size = 0.2032 MB\n",
      "print_info: arch             = bert\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 512\n",
      "print_info: n_embd           = 1024\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 16\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 1.0e-12\n",
      "print_info: f_norm_rms_eps   = 0.0e+00\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 4096\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 0\n",
      "print_info: pooling type     = 2\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 512\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 335M\n",
      "print_info: model params     = 334.09 M\n",
      "print_info: general.name     = mxbai-embed-large-v1\n",
      "print_info: vocab type       = WPM\n",
      "print_info: n_vocab          = 30522\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 101 '[CLS]'\n",
      "print_info: EOS token        = 102 '[SEP]'\n",
      "print_info: UNK token        = 100 '[UNK]'\n",
      "print_info: SEP token        = 102 '[SEP]'\n",
      "print_info: PAD token        = 0 '[PAD]'\n",
      "print_info: MASK token       = 103 '[MASK]'\n",
      "print_info: LF token         = 0 '[PAD]'\n",
      "print_info: EOG token        = 102 '[SEP]'\n",
      "print_info: max token length = 21\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA0\n",
      "load_tensors: layer  18 assigned to device CUDA0\n",
      "load_tensors: layer  19 assigned to device CUDA0\n",
      "load_tensors: layer  20 assigned to device CUDA0\n",
      "load_tensors: layer  21 assigned to device CUDA0\n",
      "load_tensors: layer  22 assigned to device CUDA0\n",
      "load_tensors: layer  23 assigned to device CUDA0\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 228 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 10 repeating layers to GPU\n",
      "load_tensors: offloaded 10/25 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =   240.51 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   637.85 MiB\n",
      ".................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_pre_seq (2048) > n_ctx_train (512) -- possible training context overflow\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    80.00 MiB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_init_from_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.00 MiB\n",
      "llama_init_from_model:      CUDA0 compute buffer size =    27.00 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
      "llama_init_from_model: graph nodes  = 849\n",
      "llama_init_from_model: graph splits = 228 (with bs=512), 2 (with bs=1)\n",
      "CUDA : ARCHS = 500,610,700,750,800 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.mask_token_id': '103', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'general.architecture': 'bert', 'bert.block_count': '24', 'bert.attention.layer_norm_epsilon': '0.000000', 'bert.context_length': '512', 'bert.feed_forward_length': '4096', 'bert.embedding_length': '1024', 'tokenizer.ggml.cls_token_id': '101', 'tokenizer.ggml.token_type_count': '2', 'bert.attention.head_count': '16', 'tokenizer.ggml.bos_token_id': '101', 'general.file_type': '1', 'general.name': 'mxbai-embed-large-v1', 'bert.attention.causal': 'false', 'bert.pooling_type': '2'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =     131.24 ms\n",
      "llama_perf_context_print: prompt eval time =     257.34 ms /   724 tokens (    0.36 ms per token,  2813.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     270.36 ms /   725 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n",
    "import os\n",
    "\n",
    "model_path = \"/home/iesus_robot/projects/langchain-lab/models/models--mixedbread-ai--mxbai-embed-large-v1/snapshots/db9d1fe0f31addb4978201b2bf3e577f3f8900d2/gguf/mxbai-embed-large-v1-f16.gguf\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model path does not exist. Please check the path.\")\n",
    "\n",
    "llama = LlamaCppEmbeddings(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,\n",
    "    verbose=True,\n",
    "    n_gpu_layers=10,  \n",
    "    n_threads=8,\n",
    ")\n",
    "db = Chroma.from_documents(documents, llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5e946fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     131.24 ms\n",
      "llama_perf_context_print: prompt eval time =      48.94 ms /    17 tokens (    2.88 ms per token,   347.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =      57.10 ms /    18 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Page: 0\n",
      "Content: Iesus Dávila Ingeniero Mecatrónico\n",
      "Teléfono: +593 96 314 5136\n",
      "Email: iesusdavila@gmail.com\n",
      "Dirección\n",
      "====================\n",
      "Page: 0\n",
      "Content: Iesus Dávila Ingeniero Mecatrónico\n",
      "Teléfono: +593 96 314 5136\n",
      "Email: iesusdavila@gmail.com\n",
      "Dirección\n",
      "====================\n",
      "Page: 0\n",
      "Content: Iesus Dávila Ingeniero Mecatrónico\n",
      "Teléfono: +593 96 314 5136\n",
      "Email: iesusdavila@gmail.com\n",
      "Dirección\n",
      "====================\n",
      "Page: 0\n",
      "Content: Unilever\n",
      "Ingeniero en Robótica e IA | Febrero 2025 - Presente\n",
      "Digital Factory Intern | Agosto 2024 -\n"
     ]
    }
   ],
   "source": [
    "query = \"Cual es el numero de telefono de Iesus?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "\n",
    "for opts in retireved_results:\n",
    "    print(\"=\"*20)  \n",
    "    print(f\"Page: {opts.metadata['page']}\")\n",
    "    print(f\"Content: {opts.page_content[:100]}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "349eed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     131.24 ms\n",
      "llama_perf_context_print: prompt eval time =     261.45 ms /   724 tokens (    0.36 ms per token,  2769.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     287.55 ms /   725 tokens\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model_path = \"/home/iesus_robot/projects/langchain-lab/models/models--mixedbread-ai--mxbai-embed-large-v1/snapshots/db9d1fe0f31addb4978201b2bf3e577f3f8900d2/gguf/mxbai-embed-large-v1-f16.gguf\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model path does not exist. Please check the path.\")\n",
    "\n",
    "db_faiss = FAISS.from_documents(documents, llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "528f31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     131.24 ms\n",
      "llama_perf_context_print: prompt eval time =     185.02 ms /    17 tokens (   10.88 ms per token,    91.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.28 ms /    18 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Page: 0\n",
      "Content: Iesus Dávila Ingeniero Mecatrónico\n",
      "Teléfono: +593 96 314 5136\n",
      "Email: iesusdavila@gmail.com\n",
      "Dirección\n",
      "====================\n",
      "Page: 0\n",
      "Content: Unilever\n",
      "Ingeniero en Robótica e IA | Febrero 2025 - Presente\n",
      "Digital Factory Intern | Agosto 2024 -\n",
      "====================\n",
      "Page: 1\n",
      "Content: INVESTIGACIÓN Software Architecture and Simulation Interface for Autonomous Underwater Vehicles\n",
      "Pape\n"
     ]
    }
   ],
   "source": [
    "query = \"Cual es el numero de telefono de Iesus?\"\n",
    "retireved_results=db_faiss.similarity_search(query)\n",
    "\n",
    "for opts in retireved_results:\n",
    "    print(\"=\"*20)  \n",
    "    print(f\"Page: {opts.metadata['page']}\")\n",
    "    print(f\"Content: {opts.page_content[:100]}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
